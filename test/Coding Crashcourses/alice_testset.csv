,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,What does the King do when he encounters Alice talking to the Cheshire Cat?,"['The Queen smiled and passed on.\n\n“Who _are_ you talking to?” said the King, going up to Alice, and\nlooking at the Cat’s head with great curiosity.\n\n“It’s a friend of mine—a Cheshire Cat,” said Alice: “allow me to\nintroduce it.”\n\n“I don’t like the look of it at all,” said the King: “however, it may\nkiss my hand if it likes.”\n\n“I’d rather not,” the Cat remarked.\n\n“Don’t be impertinent,” said the King, “and don’t look at me like\nthat!” He got behind Alice as he spoke.\n\n“A cat may look at a king,” said Alice. “I’ve read that in some book,\nbut I don’t remember where.”\n\n“Well, it must be removed,” said the King very decidedly, and he called\nthe Queen, who was passing at the moment, “My dear! I wish you would\nhave this cat removed!”\n\nThe Queen had only one way of settling all difficulties, great or\nsmall. “Off with his head!” she said, without even looking round.\n\n“I’ll fetch the executioner myself,” said the King eagerly, and he\nhurried off.']","The King expresses his dislike for the Cheshire Cat and demands it to be removed from Alice's presence. He calls the Queen, who orders the cat's execution without hesitation.",simple,"[{'source': 'alice_in_wonderland.txt', 'file_name': 'alice_in_wonderland.txt_chunk127'}]",True
1,"How many times does a learning algorithm go through the entire training dataset in one epoch?""
",['The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n\nSome recent developments..'],nan,simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
2,"
What is the mathematical formula for the hyperbolic tangent activation function?

context: Activation Functions

Linear Activation functions produce linear decisions no matter the network size

Non-linearities allow us to approximate arbitrarily complex functions

Common Activation Functions

Sigmoid Function ge) wr 0.8 ge) A 06 Pa F 0.4 / 0.2 oe ——"" 5 0 5 (@)=— Zz) = ————_ g 1+e2 g' (2) = g(a - g(2)) tf£.nn.sigmoid(z) Hyperbolic Tangent g(2) g'@) Z_ eZ I@)= ay e* g'(z) = 1-g(z)?

NOTE: All activation functions are non-linear

Rectified Linear Unit (ReLU) 5 / at) 4 9) 3 / 2 rd 1 ~-— 0 i 5 ie} 5 g (Zz) = max(0,z) 1 z>0 Ul —_— ,","['Activation Functions\n\nLinear Activation functions produce linear decisions no matter the network size\n\nNon-linearities allow us to approximate arbitrarily complex functions\n\nCommon Activation Functions\n\nSigmoid Function ge) wr 0.8 ge) A 06 Pa F 0.4 / 0.2 oe ——"" 5 0 5 (@)=— Zz) = ————_ g 1+e2 g\' (2) = g(a - g(2)) tf£.nn.sigmoid(z) Hyperbolic Tangent g(2) g\'@) Z_ eZ I@)= ay e* g\'(z) = 1-g(z)? NOTE: All activation functions are non-linear Rectified Linear Unit (ReLU) 5 / at) 4 9) 3 / 2 rd 1 ~-— 0 i 5 ie} 5 g (Zz) = max(0,z) 1 z>0 Ul —_— , g\'(2) = {3 otherwise tf.nn.relu(z)\n\nDATA\n\nWhich dataset do you want to use?\n\nRatio of training to test data: 50%\n\n—®\n\nNoise:\n\n0\n\nBatch size:\n\n10\n\n—e\n\nREGENERATE\n\nEpoch\n\nLearning rate\n\nActivation\n\nRegularization\n\n000,000\n\n0.03\n\nTanh\n\n,\n\nhttp://playground.tensorflow.org/\n\nFEATURES\n\n+ —- 2 HIDDEN LAYERS\n\nWhich properties do you want to feed in?\n\n+\n\n+ -\n\n4 neurons \\ 2 neurons ‘ The outputs are mixed with varying weights, shown by the thickness of the lines.']","The hyperbolic tangent activation function is defined as g(z) = tanh(z), where z is the input. Its derivative is given by g'(z) = 1 - tanh²(z).",simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
3,"How many times does the learning algorithm go through the entire training dataset in one iteration?""

",['The number of epochs is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n\nSome recent developments..'],nan,simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
4,"How can the use of forward gradients instead of backpropagation contribute to faster convergence in machine learning?
","['[cs.LG] 17 Feb 2022 Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differen- tiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one an compute exac and efficien ia the for- ard mode. We call this formulation the forward. gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for back- propagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training un to twice as fast in some cases\n\nActivation Functions\n\nLinear Activation functions produce linear decisions no matter the network size']",nan,simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
5,"Which forward evaluation method computes the directional derivative for a scalar-valued objective function?

or

What method provides the directional derivative when applied to a scalar-valued objective function using forward evaluations?","['[cs.LG] 17 Feb 2022 Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differen- tiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one an compute exac and efficien ia the for- ard mode. We call this formulation the forward. gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for back- propagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training un to twice as fast in some cases\n\nActivation Functions\n\nLinear Activation functions produce linear decisions no matter the network size']",The forward gradient method computes the directional derivative for a scalar-valued objective function during forward evaluations.,multi_context,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
6,"How significant is the impact of adaptive learning rates on Neural Net convergence w/mini-batches vs. traditional GD, regarding upstream & local gradients?","['(x) Qian et al.""""On the momentum term in gradient descent learning algorithms.” 1999. uchi et al.""“Adaptive Subgradient Methods for Online Léarning and Stochastic Optimization.” 201 |. Optimization.” 2014. local minimum a e & s O m 2 > =] > Qa is) Uv ct. 5 D et) 3. =) lefe) e o global minimum\n\nTraining Neural Networks with Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) 2. Loop until convergence: 3. Compute gradient, 4, Update weights, W<-W-—7 oy oe 5. Return weights Can be very computational to compute!\n\nTraining Neural Networks with Mini-batch/Stochastic Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) * 2. Loop until convergence: * 3. Pick batch of B data points 4 Compute gradient 3. Update weights, W<-— W—7n au 6. Return weights Fast to compute and a much better estimate of the true gradient!\n\n• Mini-batches lead to faster training\n\n• Can be paralelized for GPUs\n\nSample, Batch, Epoch']","Adaptive learning rates can significantly improve the convergence of neural networks when using mini-batches in comparison to traditional gradient descent (GD). In the context of mini-batch gradient descent, adaptive methods like AdaGrad and Adam update the learning rate for each weight based on the historical information of the gradients. This allows the algorithm to adjust the learning rate dynamically during training, which can lead to faster convergence and better estimates of the true gradient. The impact of adaptive learning rates is particularly important when dealing with non-stationary problems or large datasets where the upstream and local gradients may vary significantly.",multi_context,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
7,How does the product of a scalar obj fn's Jacobian & vec compare to the vec—Jprod using forward vs reverse AD methods?,"['[cs.LG] 17 Feb 2022 Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differen- tiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one an compute exac and efficien ia the for- ard mode. We call this formulation the forward. gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for back- propagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training un to twice as fast in some cases\n\nActivation Functions\n\nLinear Activation functions produce linear decisions no matter the network size', ""[cs.LG] 17 Feb 2022 Gradients without Backpropagation Atihm Giines Baydin' Barak A. Pearlmutter? Don Syme? Frank Wood‘ Philip Torr > Abstract Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differen- tiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one an compute exac and efficien ia the for- ard mode. We call this formulation the forward. gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for back- propagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training un to twice as fast in some cases forward—backward algorithm, of which backpropagation is a special case conventionally applied to neural networks. This is mainly due to the central role of scalar-valued objectives in ML, whose gradient with respect to a very large number of inputs can be evaluated exactly and efficiently with a single evaluation of the reverse mode. Reverse mode is a member of a larger family of AD algo- rithms that also includes the forward mode (Wengert, 1964), which has the favorable characteristic of requiring only a single forward evaluation of a function (i.e., not involving any backpropagation) at a significantly lower computational cost. Crucially, forward and reverse modes of AD evalu- ate different quantities. Given a function f : R” > R™, forward mode evaluates the Jacobian—vector product J fv, where J¢ ¢ R™*” and v € IR”; and revese mode evaluates the vector—Jacobian product v™ J ¢, where v € R™. For the case of f : R” > R (e.g., an objective function in ML), for- ward mode gives us Vf - v © R. the directional derivative:""]",nan,multi_context,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}, {'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
8,Which gradient descent approach is used on an individual training batch?,"['• Mini-batches lead to faster training\n\n• Can be paralelized for GPUs\n\nSample, Batch, Epoch\n\n• A sample is a single row of data. A sample may also be called an instance, an observation, an input vector, or a feature vector.\n\nThe batch size is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. A training dataset can be divided into one or more batches.\n\n• When all training samples are used to create one batch, the learning algorithm is called batch gradient descent. When the batch is the size of one sample, the learning algorithm is called (true) stochastic gradient descent. When the batch size is more than one sample and less than the size of the training dataset, the learning algorithm is called mini-batch gradient descent. In the case of mini-batch gradient descent, popular batch sizes include 32, 64, 128 and 256 samples.']",nan,reasoning,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
