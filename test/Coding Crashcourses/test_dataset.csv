,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,"What issue does the manual choice of hyper-parameters aim to prevent in training a Convolutional Neural Network?
","['* Stop training before we have a chance to overfit Under-fitting Over-fitting Legend Loss Stop training Testing here! Training Training Iterations\n\nWhat training typically does not do\n\nChoice of the hyper-parameters has to be done manually:\n\nType of activation function • Choice of architecture (how many hidden layers, their sizes) • Learning rate, number of training epochs • What features are presented at the input layer • How to regularize\n\nIt may seem complicated at first - the best way to start is to re-use some existing setup and try your own modifications.\n\nConvolutional Neural Networks  (CNNs) \n\nConvolutional Neural Networks (CNNs)\n\n• Practical for computer vision applications\n\n• Hidden layer neurons are not fully connected\n\n• Reduce number of weights (better to avoid overfitting)\n\n• Learn same features irrespective of location in image (edges, eyes, faces…) - concept of weight sharing\n\n• Use same Learning rules (Backpropagation and gradient descent)\n\nMain Layers in a CNN']",nan,simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf_chunk013'}]",True
1,"What is the difference between forward and reverse modes of automatic differentiation, and which one is more commonly used in machine learning?
","[""[cs.LG] 17 Feb 2022 Gradients without Backpropagation Atihm Giines Baydin' Barak A. Pearlmutter? Don Syme? Frank Wood‘ Philip Torr > Abstract Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differen- tiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one an compute exac and efficien ia the for- ard mode. We call this formulation the forward. gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for back- propagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training un to twice as fast in some cases forward—backward algorithm, of which backpropagation is a special case conventionally applied to neural networks. This is mainly due to the central role of scalar-valued objectives in ML, whose gradient with respect to a very large number of inputs can be evaluated exactly and efficiently with a single evaluation of the reverse mode. Reverse mode is a member of a larger family of AD algo- rithms that also includes the forward mode (Wengert, 1964), which has the favorable characteristic of requiring only a single forward evaluation of a function (i.e., not involving any backpropagation) at a significantly lower computational cost. Crucially, forward and reverse modes of AD evalu- ate different quantities. Given a function f : R” > R™, forward mode evaluates the Jacobian—vector product J fv, where J¢ ¢ R™*” and v € IR”; and revese mode evaluates the vector—Jacobian product v™ J ¢, where v € R™. For the case of f : R” > R (e.g., an objective function in ML), for- ward mode gives us Vf - v © R. the directional derivative:"", '[cs.LG] 17 Feb 2022 Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differen- tiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one an compute exac and efficien ia the for- ard mode. We call this formulation the forward. gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for back- propagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training un to twice as fast in some cases\n\nActivation Functions\n\nLinear Activation functions produce linear decisions no matter the network size']","The forward mode and reverse mode are two methods within the family of automatic differentiation algorithms. The forward mode evaluates the Jacobian-vector product J fv, where J is the Jacobian matrix and v is a vector. This method computes the gradient of a function with respect to its inputs by performing a single forward pass through the network. On the other hand, reverse mode (also known as backpropagation) evaluates the vector-Jacobian product v^T J, where v is a vector and J is the transpose of the Jacobian matrix. This method computes gradients by propagating errors backward through the network. In machine learning, reverse mode or backpropagation is more commonly used due to its central role in computing gradients for scalar-valued objectives with respect to a large number of inputs. Forward mode, while requiring only a single forward evaluation of a function and being computationally less expensive, evaluates different quantities than reverse mode.",simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf_chunk009'}, {'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
2,"
What is the purpose of regularization techniques in preventing overfitting during classification tasks?
","['FEATURES\n\n+ —- 2 HIDDEN LAYERS\n\nWhich properties do you want to feed in?\n\n+\n\n+ -\n\n4 neurons \\ 2 neurons ‘ The outputs are mixed with varying weights, shown by the thickness of the lines.\n\nThis is the output\n\nfrom one neuron.\n\nHover to see it\n\nlarger.\n\nRegularization rate\n\nProblem type\n\n0\n\nv\n\nClassification\n\nOUTPUT\n\nTest loss 0.495 Training loss 0.510\n\nColors shows data, neuron and weight values\n\n_\n\n0 Show test data\n\n0 Discretize output\n\nRegularization: Dropout\n\n¢ During training, randomly set some activations to 0 EX SSE SSX EXBZ 6S << of\n\nRegularization: Dropout\n\n* During training, randomly set some activations to 0 * Typically ‘drop’ 50% of activations in layer tf.keras.layers.Dropout (p=0.5) * Forces network to not rely on any | node x4 V1 x2 V2 x3\n\nRegularization: Early Stopping\n\n* Stop training before we have a chance to overfit Under-fitting Over-fitting Legend Loss Stop training Testing here! Training Training Iterations\n\nWhat training typically does not do']","Regularization techniques, such as Dropout and Early Stopping, are used to prevent overfitting during classification tasks. Overfitting occurs when a model learns the training data too well, including its noise and outliers, resulting in poor performance on new, unseen data. Regularization methods introduce randomness or constraints to the learning process to prevent the model from relying too heavily on any single feature or neuron. Dropout randomly sets some activations to 0 during training, forcing the network to learn alternative representations of the data. Early Stopping stops training before the model starts overfitting based on a validation set loss.",simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf_chunk012'}]",True
3,What factors contribute to the convergence of mini-batch/stochastic gradient descent in training neural networks?,"['f —| Local gradients A Upstream gradient\n\nBackpropagation: Upstream Gradient\n\nDownstream gradients Local gradients A Upstream gradient\n\nTraining Neural Networks with Adaptive Learning Rates\n\nLearning rates need not be necessarily fixed\n\nCan be made larger or smaller depending on:\n\n• how large gradient is\n\n• how fast learning is happening\n\nsize of particular weights\n\n• etc...\n\nTraining Neural Networks with Adaptive Learning Rates\n\nMomentum Adagrad Adadelta Adam RMSProp (x) Qian et al.""""On the momentum term in gradient tf. train.MomentumOptimizer descent learning algorithms.” 1999. : - uchi et al.""“Adaptive Subgradient Methods for Online tf.train.AdagradOptimizer Léarning and Stochastic Optimization.” 201 |. tf.train.AdadeltaOptimizer tf.train.AdamOptimizer Optimization.” 2014. local minimum tf£.train.RMSPropOptimizer a e & s O m 2 > =] > Qa is) Uv ct. 5 D et) 3. =) lefe) e o global minimum', '(x) Qian et al.""""On the momentum term in gradient descent learning algorithms.” 1999. uchi et al.""“Adaptive Subgradient Methods for Online Léarning and Stochastic Optimization.” 201 |. Optimization.” 2014. local minimum a e & s O m 2 > =] > Qa is) Uv ct. 5 D et) 3. =) lefe) e o global minimum\n\nTraining Neural Networks with Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) 2. Loop until convergence: 3. Compute gradient, 4, Update weights, W<-W-—7 oy oe 5. Return weights Can be very computational to compute!\n\nTraining Neural Networks with Mini-batch/Stochastic Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) * 2. Loop until convergence: * 3. Pick batch of B data points 4 Compute gradient 3. Update weights, W<-— W—7n au 6. Return weights Fast to compute and a much better estimate of the true gradient!\n\n• Mini-batches lead to faster training\n\n• Can be paralelized for GPUs\n\nSample, Batch, Epoch']",nan,simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}, {'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf_chunk006'}]",True
4,"How does a small change in one weight impact the overall loss during backpropagation training?""
","['Common Loss Functions\n\nOp), = —APW) a exp(v)) Yie{l.. .d}\n\n21 22 3 2K x SoftMax ej K z | Lika K 2 Lika & ej K z Vit ex K = | Lien & a exp(v)) probabilities\n\nCommon Loss Functions\n\nCross entropy loss can be used with models that output a probability between 0 and | f@) y 45| @ olf | xa (2! x [08| | 0 5, 8 0.6 | : X2 H : JW)= =>, log (F(x®; w)) + (1-y) log (1 — f (x®; w)) Actual Predicted Actual Predicted loss = tf.reduce_mean( tf.nn.softmax_cross_ entropy with _logits(model.y, model.pred) )\n\nLoss Functions\n\n“Visualizing the loss landscape of neural nets”. Dec 20! 7.\n\nTraining Neural Networks with Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) weights = tf.random normal (shape, stddev=signa) 2. Loop until convergence: 3. Compute gradient, 200. 4. Update weights, W<- W-—7n os) aw, weights new = weights.assign (weights - lr * grads) wn Return weights\n\nTraining Neural Networks: Backpropagation\n\nHow does a small change in one weight (ex. Wz) affect the final loss ](W)?']","During backpropagation training, each weight update is based on the gradient of the loss function with respect to that weight. A small change in one weight (Wz) will result in a corresponding change in the output of the network, which in turn affects the error between the predicted and actual labels for that input. This error propagates back through the network, causing adjustments to other weights based on their gradients. The overall loss is then recalculated after each weight update, reflecting the impact of the change in Wz.",simple,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf_chunk003'}]",True
5,"How significant is the impact of adaptive learning rates on Neural Net convergence w/mini-batches vs. traditional GD, regarding upstream & local gradients?","['(x) Qian et al.""""On the momentum term in gradient descent learning algorithms.” 1999. uchi et al.""“Adaptive Subgradient Methods for Online Léarning and Stochastic Optimization.” 201 |. Optimization.” 2014. local minimum a e & s O m 2 > =] > Qa is) Uv ct. 5 D et) 3. =) lefe) e o global minimum\n\nTraining Neural Networks with Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) 2. Loop until convergence: 3. Compute gradient, 4, Update weights, W<-W-—7 oy oe 5. Return weights Can be very computational to compute!\n\nTraining Neural Networks with Mini-batch/Stochastic Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) * 2. Loop until convergence: * 3. Pick batch of B data points 4 Compute gradient 3. Update weights, W<-— W—7n au 6. Return weights Fast to compute and a much better estimate of the true gradient!\n\n• Mini-batches lead to faster training\n\n• Can be paralelized for GPUs\n\nSample, Batch, Epoch']","Adaptive learning rates can significantly improve the convergence of neural networks when using mini-batches in comparison to traditional gradient descent (GD). In the context of mini-batch gradient descent, adaptive methods like AdaGrad and Adam update the learning rate for each weight based on the historical information of the gradients. This allows the algorithm to adjust the learning rate dynamically during training, which can lead to faster convergence and better estimates of the true gradient. The impact of adaptive learning rates is particularly important when dealing with non-stationary problems or large datasets where the upstream and local gradients may vary significantly.",multi_context,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
6,"How does deep learning's layer structure affect feature engineering compared to traditional machine learning?"" or ""In what way does deep learning's multi-layered architecture impact feature creation versus traditional ML?","['NEURAL NETWORKS OVERVIEW\n\n;\n\n|\n\nWhat is Deep Learning?\n\nARTIFICIAL INTELLIGENCE Any technique that enables computers to mimic human behavior MACHINE LEARNING Ability to learn without explicitly being programmed Deep LEARNING Extract patterns from data using neural networks\n\nSHALLOW LEARNING\n\nData Features Model Insight o- . ne, > fe: - —” tt gO; @ bbsbas\n\nTransforms the input data into only one or two successive representation layers e.g. SVM: high-dimensional non-linear projections But, complex problems require more refined representations Feature Engineering = humans manually engineer layers of representations of the data Good features -> solve the problem more elegantly, using fewer resources, less data\n\nDEEP LEARNING\n\nMachine Learning om — ii 223-Hi Input Feature extraction Classification Output Deep Learning om - P25 -B Input Feature extraction + Classification Output', 'DEEP LEARNING\n\nMachine Learning om — ii 223-Hi Input Feature extraction Classification Output Deep Learning om - P25 -B Input Feature extraction + Classification Output\n\nDeep Learning often involves tens or hundreds of successive layers of representations. Hierarchical representation learning - increasingly complex representations are developed. These intermediate incremental representations are learned jointly. Deep Learning completely automates feature engineering\n\nArtificial Neural Networks\n\n— Brain Neuron Structure — Dendrites Synapses Cell Body\n\nNeural Networks\n\nGood news:\n\nThe universal approximation theorem states that any continuous function f : [0, 1]n → [0, 1] can be approximated arbitrarily well by a neural network with at least 1 hidden layer with a finite number of weights.\n\nBad news:\n\nA feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly.']","Deep learning's multi-layered architecture allows for the complete automation of feature engineering, as it learns increasingly complex representations of data in a hierarchical manner. In contrast, traditional machine learning relies on humans to manually engineer features and transform input data into only one or two representation layers.",multi_context,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}, {'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
7,"How does the momentum term affect adaptive learning rate optimization methods for neural networks during mini-batch/stochastic gradient descent, and what sets these techniques apart from traditional gradient descent in terms of computational efficiency and convergence?""

|im_end|> The role of the momentum term in adaptive learning rate optimization methods for neural networks during mini-batch/stochastic gradient descent and how they differ from traditional gradient descent in terms of computational efficiency and convergence is an intriguing question. Could you shed some light on its implications? Specifically, what impact does the momentum term have on these optimization techniques and what advantages do they offer over traditional gradient descent?","['(x) Qian et al.""""On the momentum term in gradient descent learning algorithms.” 1999. uchi et al.""“Adaptive Subgradient Methods for Online Léarning and Stochastic Optimization.” 201 |. Optimization.” 2014. local minimum a e & s O m 2 > =] > Qa is) Uv ct. 5 D et) 3. =) lefe) e o global minimum\n\nTraining Neural Networks with Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) 2. Loop until convergence: 3. Compute gradient, 4, Update weights, W<-W-—7 oy oe 5. Return weights Can be very computational to compute!\n\nTraining Neural Networks with Mini-batch/Stochastic Gradient Descent\n\nAlgorithm |. Initialize weights randomly ~NV(0, 07) * 2. Loop until convergence: * 3. Pick batch of B data points 4 Compute gradient 3. Update weights, W<-— W—7n au 6. Return weights Fast to compute and a much better estimate of the true gradient!\n\n• Mini-batches lead to faster training\n\n• Can be paralelized for GPUs\n\nSample, Batch, Epoch']","The momentum term is a technique used in adaptive learning rate optimization methods for neural networks during mini-batch/stochastic gradient descent to improve convergence. It helps mitigate the problem of oscillations or getting stuck at local minima by adding a fraction of the previous weight update to the current one, effectively smoothing out the learning process. This can lead to faster training and better estimates of the true gradient compared to traditional gradient descent. Additionally, mini-batch/stochastic gradient descent techniques are more computationally efficient as they only require computing gradients for a small batch of data points at each iteration instead of the entire dataset, making them suitable for parallelization on GPUs.",multi_context,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf_chunk006'}]",True
8,What role do activation functions play when comparing forward gradient & backpropagation techniques?,"['[cs.LG] 17 Feb 2022 Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differen- tiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one an compute exac and efficien ia the for- ard mode. We call this formulation the forward. gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for back- propagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training un to twice as fast in some cases\n\nActivation Functions\n\nLinear Activation functions produce linear decisions no matter the network size']","Activation functions determine the non-linearity of a neural network, allowing it to model complex relationships between inputs and outputs. In the context of forward gradient and backpropagation techniques, they are used during both methods for computing gradients. However, the main difference lies in how these gradients are computed: forward gradient computes an unbiased estimate of the gradient using only a single forward run of the function, while backpropagation requires multiple backward passes to compute the gradients.",reasoning,"[{'source': '09_week8_neural_networks.pdf', 'file_name': '09_week8_neural_networks.pdf'}]",True
