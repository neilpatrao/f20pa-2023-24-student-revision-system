question,ground_truth
What are some common activation functions?,"Sigmoid Function, Hyperbolic Tangent, Rectified Linear Unit (ReLU)"
What are some common loss functions?,"Mean squared error, SoftMax, Cross entropy"
Who introduced word2vec and when?,"Word2vec was introduced in 2013 by a team of researchers led by Thomas Mikolov at Google."
What is the difference between stemming and lemming?,"Stemming replaces each word with its word stem using a rule-based heuristic, while lemmatization replaces each word with its standardized form using a dictionary of known word forms."
What are stopwords and why do we need to remove them?,"Stopwords are common words in a language that are often removed during text processing to reduce features in documents, improve efficiency, and eliminate words that are not useful for searching or text mining."
What is clustering?,"Clustering is a method of grouping instances of data into meaningful clusters based on their similarity or distance from each other."
Who introduced Latent Dirichlet Allocation and when?,"Latent Dirichlet Allocation (LDA) was introduced by David Blei, Andrew Ng, and Michael Jordan in 2003."
What is Bag-Of-Words and how does it work?,"The Bag-of-Words (BoW) representation is a method commonly used to represent text for machine learning. Each document is represented as a vector in a vector space model, with one dimension for every unique term in the space. BoW discards most of the structure of the input text, such as chapters, paragraphs, sentences, and formatting, and only counts how often each word appears in each text. This representation does not consider the ordering of words in a document but works well for many applications like classification and topic modeling."
How tall is Mount Everest?,"Unable to answer the question using the provided context."
Who is Neil Armstrong?,"Unable to answer the question using the provided context."
What are recurrent neural networks?,"Recurrent Neural Networks (RNNs) are sequential models that perceive text as a sequence of characters or words. Unlike other neural network models, RNNs can take sequential input of any length and share parameters over time. They are capable of modeling sequence and long-term dependencies in text. However, RNNs face challenges like vanishing or exploding gradients, which are addressed by variations such as LSTM and GRU, which use gates to control information flow and add the notion of memory."
What is language modeling?,"Language modeling is the task of predicting what word comes next in a sequence of words. It involves computing the probability of a sentence or sequence of words (P(W)) or the probability of an upcoming word given the previous words (P(wn|w1,w2,...,wn-1)). A model that computes either of these probabilities is called a language model."
What are vanishing/exploding gradients and how can it be solved?,"Vanishing gradients occur when the gradient effect from earlier states is lost, making it difficult for the network to learn long-term dependencies. This problem is more difficult than exploding gradients, which happen when the derivative grows exponentially as a function of the layer. Vanishing gradients can be addressed by techniques like using LSTM and GRU cells. Exploding gradients can be solved by gradient clipping."
Which is better GRU or LSTM?,"Both LSTM (Long Short Term Memory) and GRU (Gated Recurrent Unit) cells are widely used in neural networks for handling sequential data. While LSTMs are designed to maintain a cell state where information can flow easily and have various gates to control the flow of information, GRUs are simplified versions of LSTMs with similar performance, where both state vectors are merged into a single vector. Therefore, the choice between LSTM and GRU often depends on the specific requirements and constraints of the task at hand."
What are the two components of the transformer architecture?,"The two components of the transformer architecture are encoders and decoders."
What is attention?,"Attention is a mechanism used in neural machine translation and deep learning, revolutionizing the field by significantly improving the state of the art, particularly for long sentences. It allows the encoder to produce a hidden state at each step that the decoder can access, rather than a single hidden state for the entire input sequence."
What are the two catagories of transformer models?,"The two categories of transformer models are Encoder-only and Decoder-only. The Encoder-only models are used to convert an input sequence of text into a rich numerical representation, suitable for tasks like text classification or named entity recognition. Examples include BERT and its variants like RoBERTa and DistilBERT. On the other hand, Decoder-only models, such as the family of GPT models, autocomplete the sequence by iteratively predicting the most probable next word."
What are the applications of transformer models?,"The applications of transformer models include machine translation, summarization tasks, text classification, named entity recognition, autocompletion of sequences, and processing multiple modalities like images, videos, text, and speech."
What are the two types of hierarchical clustering?,"The two types of hierarchical clustering are agglomerative (bottom-up) clustering and divisive (top-down) clustering."
How does cosine similarity work?,"Cosine similarity works by computing the cosine of the angle between two vectors representing documents."