{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Ragas Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Loading Lecture Materials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n",
      "This function will be deprecated in a future release and `unstructured` will simply use the DEFAULT_MODEL from `unstructured_inference.model.base` to set default model name\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.directory import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\"lectures\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Test Dataset Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(question):\n",
    "\n",
    "    template = \"\"\"For each question and context, create an answer only using the context. If the context does not contain the answer, say you are unable to answer the question using the context. Do not make up an answer.\n",
    "\n",
    "    answer: answer to the question using the context.\n",
    "\n",
    "    question: {question}\n",
    "\n",
    "    context: {context}\"\"\"\n",
    "\n",
    "    docs = vectorstore.similarity_search(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    return template.format(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For each question and context, create an answer only using the context. If the context does not contain the answer, say you are unable to answer the question using the context. Do not make up an answer.\\n\\n    answer: answer to the question using the context.\\n\\n    question: Who is Neil Armstrong?\\n\\n    context: The bag of words representation\\n\\nγ(\\n\\n)=c\\n\\nF20/F21 AA Applied Text Analytics\\n\\n6\\n\\nThe bag of words representation\\n\\nI love this movie! It\\'s sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun…  It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I\\'ve seen it several times, and I\\'m always happy to see it again whenever I have a friend who hasn\\'t seen it yet.\\n\\nγ(\\n\\n)=c\\n\\nF20/F21 AA Applied Text Analytics\\n\\n7\\n\\nThe bag of words representation: \\x0busing a subset of words\\n\\nx love xxxxxxxxxxxxxxxx sweet xxxxxxx satirical xxxxxxxxxx xxxxxxxxxxx great xxxxxxx xxxxxxxxxxxxxxxxxxx fun  xxxx xxxxxxxxxxxxx whimsical xxxx romantic xxxx  laughing xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx recommend xxxxx xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx several xxxxxxxxxxxxxxxxx xxxxx  happy xxxxxxxxx again xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\n\\nγ(\\n\\n)=c\\n\\nF20/F21 AA Applied Text Analytics\\n\\n10 January 2023 \\n\\n“Language models write credibly, but not necessarily the truth”\\n\\nChris presents a nice history on the development of the NLP and comment on the current  breakthroughs \\n\\n“\\xa0in the late 1980s, people tried to teach language to computers by introducing rules, grammar, and logic into those computers.\\xa0…digital texts became available, such as court proceedings and newspaper archives… the approach changed statistical NLP. Then that merged with machine learning, a form of artificial intelligence that can discover patterns on its own in a huge amount of data\\n\\n…………….. In 2018, they discovered that if you take a huge amount of human language and train a giant neural network to predict words, you get a model that can do much more…..\\n\\nRight now, computers learn language by reading a huge amount of text and learning to predict what people are saying or writing”\\n\\nListen to interview May 2023  https://www.youtube.com/watch?v=biN5yufyLZM \\n\\nIntroduction to Text Analytics\\n\\n65\\n\\nF20 21 AA Applied Text Analytics\\n\\nRNN Encoder Decoder(with attention mechanism) \\n\\nUse a different context vector in each timestep of decoder\\n\\nx x x x rs 4 r 4 az1 a2 423 Ang soTymax t ii t t C71 C22 C23 C24 X1 X> X3 X4 we are eating bread in each timestep estamos comiendo | Ci || Yo Co | | V1 [START] estamos\\n\\nF20 21 AA Applied Text Analytics\\n\\nRNN Encoder Decoder (with attention mechanism) \\n\\nagreement on European Economic Area was signed in A xe} c o Vv The the August 1992 L accord sur la zone économique européenne D.\\n\\n• decoder assign a different amount of weight, or “attention,” to each of the encoder states\\n\\nEach pixel denotes a weight. • attention-based models are able to learn\\n\\nnontrivial alignments between the words in a generated translation and those in a source sentence\\n\\nD. Bahdanau, K. Cho, and Y. Bengio, “Neural Machine Translation by Jointly Learning to Align and Translate” (2014).\\n\\nthe source sentence (English) and the generated translation (French),\\n\\nRNN popular model that has been used in NLP.\\n\\nRNN is a sequen5al model that can perceive the text as a sequence of characters & words.\\n\\nF20 F21 AA Applied Text Analytics\\n\\nRNN (Recurrent Neural Network) \\n\\nF20 F21 AA Applied Text Analy4cs\\n\\nRNN (Recurrent Neural Network) \\n\\n• They can take sentences, documents, or audio samples as input, making them extremely useful for natural language processing (NLP) systems such as automa=c transla=on, speech-to-text, or sen=ment analysis.\\n\\n• (Unlike other NN models):\\n\\nTake sequen*al input of any length • Share parameters over *me\\n\\n• *RNNs’ ability to an=cipate also makes them capable of surprising crea=vity.\\n\\ncompose a melody such as the one produced by Google’s Magenta project. • Generate sentences, image cap*ons, and much more\\n\\nF20 F21 AA Applied Text Analytics\\n\\nHUAWEI THE UNFINISHED SYMPHONY > Pl 3138/4500 Franz Schubert - Symphony No.8 in B minor, D.759 (\"Unfinished\") finalized by artificial intelligence'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_prompt(\"Who is Neil Armstrong?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", temperature=0)\n",
    "llm_temp07 = ChatOpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\", temperature=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# template = \"\"\"Answer the question based only on the following context:\n",
    "# {context}\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\"\n",
    "template = \"\"\"Answer the question using the context.\n",
    "\n",
    "question: {question}\n",
    "context: {context}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\",\"question\"]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain_temp07 = (\n",
    "    {\"context\": retriever | format_docs,  \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_temp07\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are some common activation functions?</td>\n",
       "      <td>Sigmoid Function, Hyperbolic Tangent, Rectifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are some common loss functions?</td>\n",
       "      <td>Mean squared error, SoftMax, Cross entropy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who introduced word2vec and when?</td>\n",
       "      <td>Word2vec was introduced in 2013 by a team of r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the difference between stemming and le...</td>\n",
       "      <td>Stemming replaces each word with its word stem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are stopwords and why do we need to remov...</td>\n",
       "      <td>Stopwords are common words in a language that ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0         What are some common activation functions?   \n",
       "1               What are some common loss functions?   \n",
       "2                  Who introduced word2vec and when?   \n",
       "3  What is the difference between stemming and le...   \n",
       "4  What are stopwords and why do we need to remov...   \n",
       "\n",
       "                                        ground_truth  \n",
       "0  Sigmoid Function, Hyperbolic Tangent, Rectifie...  \n",
       "1         Mean squared error, SoftMax, Cross entropy  \n",
       "2  Word2vec was introduced in 2013 by a team of r...  \n",
       "3  Stemming replaces each word with its word stem...  \n",
       "4  Stopwords are common words in a language that ...  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testset = pd.read_csv(\"testset.csv\", nrows=10)\n",
    "testset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "questions = testset[\"question\"].to_list()\n",
    "ground_truth = testset[\"ground_truth\"].to_list()\n",
    "\n",
    "def generate_dataset(rag_chain):\n",
    "\n",
    "    data = {\"question\": [], \"answer\": [], \"contexts\": [], \"ground_truth\": ground_truth}\n",
    "\n",
    "    for query in tqdm(questions):\n",
    "        data[\"question\"].append(query)\n",
    "        data[\"answer\"].append(rag_chain.invoke(query).strip())\n",
    "        data[\"contexts\"].append([doc.page_content for doc in retriever.get_relevant_documents(query)])\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Mistral 7b Instruct**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Temperature = 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:30<00:00,  3.01s/it]\n"
     ]
    }
   ],
   "source": [
    "mistral_dataset = generate_dataset(rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▎         | 1/40 [00:06<03:58,  6.11s/it]No statements were generated from the answer.\n",
      "No statements were generated from the answer.\n",
      "Evaluating: 100%|██████████| 40/40 [01:01<00:00,  1.55s/it]\n"
     ]
    }
   ],
   "source": [
    "mistral_result = evaluate(mistral_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.7320, 'context_precision': 0.7833, 'faithfulness': 1.0000, 'context_recall': 0.7667}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Temperature = 0.7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:31<00:00,  3.18s/it]\n"
     ]
    }
   ],
   "source": [
    "mistral_dataset_temp07 = generate_dataset(rag_chain_temp07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  32%|███▎      | 13/40 [00:07<00:10,  2.51it/s]No statements were generated from the answer.\n",
      "Evaluating:  98%|█████████▊| 39/40 [01:05<00:04,  4.23s/it]No statements were generated from the answer.\n",
      "Evaluating: 100%|██████████| 40/40 [01:09<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "mistral_result_temp07 = evaluate(mistral_dataset_temp07)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.6569, 'context_precision': 0.7833, 'faithfulness': 0.9250, 'context_recall': 0.7667}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistral_result_temp07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dolphin 7b Mistral**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:20<00:00,  2.04s/it]\n"
     ]
    }
   ],
   "source": [
    "dolphin_dataset = generate_dataset(rag_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]No statements were generated from the answer.\n",
      "Evaluating:  80%|████████  | 32/40 [00:40<00:16,  2.06s/it]No statements were generated from the answer.\n",
      "Evaluating: 100%|██████████| 40/40 [01:15<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "dolphin_result = evaluate(dolphin_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.8426, 'context_precision': 0.7833, 'faithfulness': 1.0000, 'context_recall': 0.7667}"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dolphin_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unstructured_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
