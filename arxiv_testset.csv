,question,contexts,ground_truth,evolution_type,metadata,episode_done
0,How can the relevance of input queries and documents in the datastore be defined for extending the use of retrieval-augmented LMs beyond conventional knowledge-centric tasks?,"['\nExtending\nthe use of retrieval-augmented LMs beyond conventional\nknowledge-centric tasks necessitates the formulation of a\nnew definition for “relevance” of the input query and docu-\nments in the datastore. This is essential for excelling in tasks\nas in those tasks informative text may not exhibit semantic\nor lexical similarity to the input query. Recent works show\nthat few-shot in-context learning demonstrations (Su et al.,\n2023a) or even unlabeled text (Lyu et al., 2023) could boost\nmodel performance on reasoning or language understand-\ning tasks. Yet, what makes certain documents helpful (e.g.,\nunderlying reasoning patterns, or writing style) remains an\nopen question. Acquiring a better understanding of the char-\nacteristics of helpful documents could unlock the potential\nof retrieval-augmented LMs. Furthermore, built upon such\nunderstanding, we should build retrieval systems capable of\ncontextualized retrieval, rather than building task-specific re-\ntrieval pipelines: developing a versatile retriever that adjusts\nits search behavior based on diverse notions of similarity\nwith additional input.\nFor instance, instruction-tuned re-\ntrievers (Asai et al., 2023b; Su et al., 2023b) exemplify this\ndirection.\nReconsidering and improving the datastore.\nWhen it\ncomes to wider, general downstream applications, or con-\nversely more expert-domain tasks, over-reliance on a single,\n7\nReliable, Adaptable, and Attributable Language Models with Retrieval\ngeneral-domain corpus such as Wikipedia may hinder the\ncapability of retrieval-augmented LMs. As discussed in\nSection 4.1.3, the curation and composition of the datas-\ntore significantly impact the final performance. Yet, many\nopen questions exist regarding how to build and ensure high-\nquality and effective datastores. For instance, should we\nintroduce a quality filter to the documents in the datastore,\nas common practice in pre-training data processing (Black\net al., 2022)? How should we balance multiple domains in a\ndatastore (Shao et al., 2023)? Despite the abundance of liter-\nature on what constitutes good LM pre-training data (Long-\npre et al., 2023), there have been limited explorations so far\non what data ought to go into the datastore.\n5.2. Enhancing Retriever-LM Interactions (C2)\nNew architectures beyond input augmentation.\nAs dis-\ncussed, the input augmentation of powerful LMs (e.g., RAG)\ncomes with several limitations that could be addressed by\nmore specialized, integrated architectures, such as output\ninterpolation or intermediate fusion. While recent work\nshows the success of new architectures (Wang et al., 2023b;\nMin et al., 2023b; Lan et al., 2023), compared to massively\npre-trained parametric LMs, their training and model size\nare often smaller, due to high computational costs for pre-\ntraining. Furthermore, approaches that employ a smaller\ngranularity of retrieval (e.g., token level in Section 4.1.1)\npose significant challenges for scaling. We urge collabora-\ntive efforts for scalable, effective architecture designs and\npre-training—While pre-training retrieval-augmented LMs\nis computationally expensive, we hope that we can address\nthat challenge through collaborative multi-institution efforts,\nas in several successful parametric LM pre-training (Work-\nshop et al., 2022; Groeneveld et al., 2024). Recently, Muen-\nnighoff et al. (2024) have introduced generative represen-\ntational instruction tuning to train a single model for both\nretrieval and generative tasks, which allows for significantly\nreducing the latency of RAG by caching representations.\nIncorporating retrieval during LM pre-training.\nOff-\nthe-shelf parametric LMs trained without retrieval compo-\nnents often struggle with leveraging additional context (Shi\net al., 2023a). Pre-training LMs with retrieval has proven to\nbe effective (Guu et al., 2020; Lewis et al., 2020a; Izacard\net al., 2023), but often requires significant additional train-\ning costs, or non-trivial modifications to the standard LM\narchitecture. Recently, Shi et al. (2024) shows that retriev-\ning similar text chunks and reordering pre-training corpora\ncan enhance LMs’ abilities to reason over long sequences or\nperform retrieval augmentation for']","The formulation of a new definition for ""relevance"" of the input query and documents in the datastore is necessary for extending the use of retrieval-augmented LMs beyond conventional knowledge-centric tasks.",simple,"[{'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}]",True
1,What is the purpose of relevance labels in the fine-tuning process?,"['-or-false\nquestions. Claims are represented about health\nwith factual information, and the model is tasked\nto verify the authenticity and give the judgment.\nAccuracy was adopted as the evaluation metric.\nArc-Challenge (Bhakthavatsalam et al., 2021)\nis a multiple-choice question task about some\ndaily commonsense science phenomena. Given\na scientific event that occurs in daily life, the model\nis required to select the correct description among\n3 or 4 optional choices. Accuracy was adopted as\nthe evaluation metric as well.\nB.2\nImplementation Details\nRetrieval Evaluator: We fine-tuned the retrieval\nevaluator based on the lightweight T5-large (Raffel\net al., 2020) pre-trained model. Its parameter size\nis much smaller than the most current LLMs (Tou-\nvron et al., 2023a,b; Chowdhery et al., 2023; Anil\net al., 2023; Brown et al., 2020; Ouyang et al.,\n2022; OpenAI, 2023). To ensure all experimental\nresults were comparable with Self-RAG (Asai et al.,\n2023), the same retrieval results through Con-\ntriever (Izacard et al., 2022) were provided by Self-\nRAG and were also adopted in our experiments.\nThe relevance signals for fine-tuning the evaluator\ncan be collected from the existing datasets. For\nexample, PopQA (Mallen et al., 2023) provides\nthe golden subject wiki title from wikipedia for\neach question.\nWe can use that to track a not\n100% relevant but rather high-quality passage. We\nutilized that as the relevance labels for fine-tuning\nthe retrieval evaluator.3 On the other hand, the\nnegative samples were randomly sampled and we\nused the version provided by Self-RAG (Asai\net al., 2023).\nSpecifically, the original PopQA\ndataset consists of 14k samples, 1,399 of which\nwere used for testing following Self-RAG (Asai\net al., 2023), and the remaining were used for\nfine-tuning to avoid information leakage. Besides,\nthe fine-tuned evaluator was transferred and also\nutilized on the Bio, Pub and ARC datasets during\ninference. The label of positive samples was 1,\nwhile that of negative ones was -1. At inference,\nthe evaluator scored the relevance from -1 to 1 for\neach document. The two confidence thresholds\nfor triggering one of the three actions were set\nempirically. Specifically, they were set as (0.59,\n-0.99) in PopQA, (0.5, -0.91) in PubQA and Arc-\nChallenge, as well as (0.95, -0.91) in Biography.\nInternal Knowledge: To obtain fine-grained\nretrieval results, we segmented the retrieved results\ninto internal strips. If a retrieved result is as short as\none or two sentences, it is regarded as an individual\nstrip, otherwise, retrieval documents are required to\nbe split into smaller units which generally consist\nof a few sentences according to the total length.\nThe scale is assumed to include an independent\npiece of information, and the filtering is based on\nthe segments. We directly adopted the evaluator\nagain for knowledge strips filtering, and the top-k\nis set to 5, filter threshold as -0.5.\nExternal Knowledge:\nGoogle Search API\nwas adopted to search for the relevant URLs,\ntop-k is set to 5, and pages from Wikipedia\nwill be added preferentially. The searched web\npages are generally in the form of HTML files,\nwhere content is split with special tokens like\n<p> and </p>. Thus an extra segmentation like\nthe knowledge refinement is not required, related\nknowledge paragraphs can be directly selected with\nthe evaluator similar to internal knowledge.\nGenerator:\nAs CRAG is a plug-and-play\nmethod, all generation models that can be uti-\nlized in RAG fit our approach as well.\nTo\nbe consistent with baselines for comparison, we\nadopted LLaMA2 (Touvron et al., 2023b) for the\ngeneration. We first introduced the LLaMA2-hf-\n7b from huggingface to generate responses. Since\nSelf-RAG (Asai et al., 2023) fine-tuned LLaMA2\nand reached a new state-of-the-art performance\n3https://huggingface.co/datasets/akariasai/PopQA\non several']",nan,simple,"[{'Published': '2024-02-16', 'Title': 'Corrective Retrieval Augmented Generation', 'Authors': 'Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling', 'Summary': 'Large language models (LLMs) inevitably exhibit hallucinations since the\naccuracy of generated texts cannot be secured solely by the parametric\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\ndocuments, raising concerns about how the model behaves if retrieval goes\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\nretrieval evaluator is designed to assess the overall quality of retrieved\ndocuments for a query, returning a confidence degree based on which different\nknowledge retrieval actions can be triggered. Since retrieval from static and\nlimited corpora can only return sub-optimal documents, large-scale web searches\nare utilized as an extension for augmenting the retrieval results. Besides, a\ndecompose-then-recompose algorithm is designed for retrieved documents to\nselectively focus on key information and filter out irrelevant information in\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\nRAG-based approaches. Experiments on four datasets covering short- and\nlong-form generation tasks show that CRAG can significantly improve the\nperformance of RAG-based approaches.'}]",True
2,What is the concept of unsupervised dense information retrieval?,"['-main.74.\nIzacard, G. and Grave, E.\nDistilling knowledge from\nreader to retriever for question answering.\nIn In-\nternational Conference on Learning Representations,\n2021b. URL https://openreview.net/forum?\nid=NTEz-6wysdb.\nIzacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,\nP., Joulin, A., and Grave, E. Unsupervised dense infor-\nmation retrieval with contrastive learning. Transactions\non Machine Learning Research, 2022. URL https:\n//openreview.net/forum?id=jKN1pXi7b0.\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,\nF., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and\nGrave, E. Atlas: Few-shot learning with retrieval aug-\nmented language models. Journal of Machine Learning\nResearch, 2023. URL http://jmlr.org/papers/\nv24/23-0037.html.\nJang, J., Yoon, D., Yang, S., Cha, S., Lee, M., Logeswaran,\nL., and Seo, M. Knowledge unlearning for mitigating pri-\nvacy risks in language models. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics, 2023. URL https://aclanthology.\norg/2023.acl-long.805.\nJiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J.,\nYang, Y., Callan, J., and Neubig, G. Active retrieval aug-\nmented generation. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 2023.\nURL https://aclanthology.org/\n2023.emnlp-main.495.\nJin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei, X.,\nArnold, A., and Ren, X. Lifelong pretraining: Continually\nadapting language models to emerging corpora. In Pro-\nceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2022. URL https://\naclanthology.org/2022.naacl-main.351.\nJohnson, J., Douze, M., and J´\negou, H. Billion-scale similar-\nity search with gpus. arXiv preprint arXiv:1702.08734,\n2017.\nURL https://arxiv.org/abs/1702.\n08734.\nKandpal, N., Deng, H., Roberts, A., Wallace, E.,\nand Raffel, C.\nLarge language models struggle\nto learn long-tail knowledge.\nIn International\nConference on Machine Learning, 2022a.\nURL\nhttps://proceedings.mlr.press/v202/\nkandpal23a/kandpal23a.pdf.\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating train-\ning data mitigates privacy risks in language models. In\nInternational Conference on Machine Learning, 2022b.\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L.,\nEdunov, S., Chen, D., and Yih, W.-t. Dense passage\nretrieval for open-domain question answering. In Pro-\nceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, 2020. URL https://\naclanthology.org/2020.emnlp-main.550.\nKasai, J., Sakaguchi, K., Takahashi, Y., Bras, R. L., Asai,\nA., Yu, X., Radev, D., Smith, N. A., Choi, Y., and Inui,\nK. Realtime qa: What’s the answer right now? In Thirty-\nseventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2023. URL\nhttps://arxiv.org/abs/2207.13332.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer,']",Unsupervised dense information retrieval refers to a method of retrieving information from a large dataset without the need for labeled training data. It involves using contrastive learning techniques to learn representations of the data and then using these representations to retrieve relevant information.,simple,"[{'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}]",True
3,What are some challenges associated with factual inaccuracies in parametric language models?,"[' masked tokens. Dur-\ning test time, for an input sequence x, the trained θ predicts\nthe outputs: y = fθ(x), without accessing any external data\nbeyond that of the task at hand.\n2.1. Weaknesses of Parametric LMs\nMounting evidence highlights significant limitations in para-\nmetric LMs. Many such challenges arise from the strategy\nof attempting to store all knowledge within the parameters,\nwhich scaling alone may not adequately address.\nW1: Factual inaccuracies.\nAttempting to memorize all\nthe learned knowledge within the parameters can lead to\nfactual inaccuracies, which are often called hallucinations.\nSeveral recent papers report that even state-of-the-art LMs\nsuch as ChatGPT exhibit hallucinations in the majority of\ntheir outputs (Min et al., 2023a; Mishra et al., 2024). Mallen\net al. (2023); Kandpal et al. (2022a) show that they particu-\nlarly struggle with long-tail knowledge—factual knowledge\nthat is less represented during pre-training—and that scaling\nonly yields minor improvements. Gudibande et al. (2024)\nfind that increasing synthetic labeled data during instruction\ntuning may not improve the factuality of model outputs.\nW2: Difficulty of verifications.\nNot only have LMs\nshown a propensity for hallucinations in their generations,\nbut it is also difficult for practitioners to fact-check their out-\nputs due to a lack of clear attributions or provenance. The\noutputs of powerful LMs are often lengthy, assertive, and\nplausible (Min et al., 2023a), which makes post-hoc attri-\nbutions or factual verification to be challenging and largely\nunsolved tasks (Mishra et al., 2024; Yue et al., 2023).\nW3: Difficulty of opting out certain sequences from the\ndatasets.\nManaging the vast volume of pre-training data\nposes a considerable challenge in identifying and filtering\nout training instances with potential privacy (Brown et al.,\n2022) or copyright-protected data (Lee et al., 2024). Re-\ncent work studies intensive red teaming and safety tuning\nefforts (Touvron et al., 2023b; Perez et al., 2022), unlearn-\ning (Jang et al., 2023) or iterative pre-training of models on\ncorpora after removing certain data (Kandpal et al., 2022b).\nYet, the absence of proper attributions further complicates\nthese endeavors, as tracing back to and eliminating specific\ntraining instances becomes non-trivial (Grosse et al., 2023).\nW4: Computationally expensive costs to adapt.\nAdapt-\ning parametric LMs trained on static unlabeled text (i.e.,\n2\nReliable, Adaptable, and Attributable Language Models with Retrieval\ntext collected at a certain timestamp from the web) re-\nquires continuous training or computationally expensive\npost-adaptation to new data distributions. For instance, their\nparametric knowledge can quickly become obsolete (Long-\npre et al., 2023). While several approaches propose to locate\nand edit certain outdated knowledge (De Cao et al., 2021)\nor conduct efficient continued training (Jin et al., 2022) to\nkeep up with the world, these approaches require additional\ncomputationally expensive learning processes. LMs trained\non widely adopted pre-training corpora often perform well\non general-purpose domains such as news articles (Dodge\net al., 2021), but struggle on expert domains (Taylor et al.,\n2022). Prior work demonstrates the effectiveness of contin-\nued pre-training (Azerbayev et al., 2024; Chen et al., 2023b)\nor instruction tuning (Singhal et al., 2023), albeit at a consid-\nerable computational cost and possibilities of catastrophic\nforgetting (Li et al., 2022).\nW5: Prohibitively large model size.\nNumerous studies\nshowcase the positive impact of model scaling on task per-\nformance (Chowdhery et al., 2022; Wei et al., 2022), and\nthe ability to recall factual knowledge memorized from the\ntraining data (Carlini et al., 2023; Mallen et al., 2023; Kand-\npal et al., 2022a). This trend has prompted the community\nto focus on boosting the model size in pursuit of better\nperformance, at the cost of significant computational chal-\nlenges and environmental concerns (Strubell et al., 2019;\nWeidinger et al., 2022). Despite efforts to enhance effi-\nciency, hosting these massive models, which often exceed a', 'ed LMs\nto supersede parametric LMs as the next generation of LMs\n(Figure 1, bottom), addressing many of the aforementioned\nweaknesses. Unlike parametric LMs—which use large-scale\ntext data only during training—retrieval-augmented LMs\nleverage an external large-scale collection of documents\n(datastore) at inference by selecting relevant documents\nfrom the datastore (Asai et al., 2023a). Retrieval-augmented\nLMs can W1: largely reduce factual errors (Mallen et al.,\n2023), W2: provide better attributions (Gao et al., 2023a),\nW3: enabling flexible opt-in and out of sequences (Min\net al., 2024). By adding or removing data from their datas-\ntores, retrieval-augmented LMs can W4: easily adapt to new\ndistributions (Khandelwal et al., 2020). Lifting the burden\nof memorizing everything in parameters makes them W5:\n1\narXiv:2403.03187v1  [cs.CL]  5 Mar 2024\nReliable, Adaptable, and Attributable Language Models with Retrieval\nmore parameter-efficient (Izacard et al., 2023).\nDespite their considerable potential to significantly improve\nreliability, adaptability, and attributability, their broader\nadoption beyond specific knowledge-intensive tasks (e.g.,\nquestion answering or QA; Chen et al. 2017) is currently\nlimited. We argue that through fundamental advancements\nin architecture, training methodologies, and infrastructure\nfor retrieval-augmented LMs, they can demonstrate sub-\nstantial efficacy across diverse domains. We urge the re-\nsearch community to intensify efforts aimed at overcoming\nthose inherent limitations for their widespread adoption.\nTo facilitate future research, we identify several significant\nchallenges. First, existing approaches primarily leverage\ncontext with high semantic or lexical similarity to the in-\nput (C1), struggling when helpful text is absent in com-\nmon datastores or does not align with conventional rele-\nvance definitions (BehnamGhader et al., 2023; Asai et al.,\n2023b). Second, prepending the retrieved text to the in-\nput of off-the-shelf LMs, which has been widely used re-\ncently, leads to shallow interactions between the retrieval\nand LM components (C2). This often results in unsupported\ngenerations (Gao et al., 2023a), susceptibility to irrelevant\ntext (Yoran et al., 2024), and challenges in handling infor-\nmation from multiple pieces of text (Borgeaud et al., 2022).\nFurthermore, unlike rapid progress for efficient training and\ninference of parametric LMs (Zhao et al., 2023b; Dao et al.,\n2022), there are limited studies and open-sourced efforts to\nenhance the training and inference efficiency of retrieval-\naugmented LMs at scale (C3).\nWe conclude this paper with a roadmap to advance retrieval-\naugmented LMs to foster wider adoption. First, addressing\nthe challenge of finding helpful text for diverse tasks (C1),\nit is important to reconsider the notion of relevance and\nadvance our understanding of what constitutes an effec-\ntive datastore—specifically, exploring the types of infor-\nmation that should be retrieved from various datastores to\nenhance the performance in broader tasks. Then, we sug-\ngest approaches to ensure deeper interactions between the\ntwo components, including architecture, pre-training, and\npost-training adaptations (C2), rather than focusing on sup-\nplementary enhancement of existing parametric LMs. For\nchallenges of scaling (C3), we call for more open-sourced\nand interdisciplinary efforts across hardware, systems, and\nalgorithms to develop infrastructures for training and infer-\nence (e.g., scaling datastore to trillion tokens). By pursuing\nthese avenues, we anticipate unlocking the full capabilities\nof retrieval-augmented LMs and expanding their applica-\ntions across a spectrum of tasks and domains.\n2. How Far Can We Go with Parametric LMs?\nWe first assess the limitations of parametric LMs. Despite\nrapid progress in this area, we argue that parametric LMs\nhave many practical limitations, which in turn pose signifi-\ncant challenges to building reliable intelligent systems.\nDefinition.\nA parametric LM (Figure 1, top) consists of\na set of parameters θ. Given input sequences from a large-\nscale text dataset Dtrain, learnable parameters θ are trained\nto predict the probabilities of future or']","Factual inaccuracies in parametric language models can lead to hallucinations and incorrect information in their outputs. Even state-of-the-art LMs like ChatGPT exhibit hallucinations in the majority of their outputs. Scaling alone may not adequately address this issue, especially with long-tail knowledge that is less represented during pre-training. Increasing synthetic labeled data during instruction tuning may also not improve the factuality of model outputs.",simple,"[{'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}, {'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}]",True
4,How has retrieval-augmented generation been adapted to the task of data-to-text generation?,"[' edited template. Xiao et al. (2021)\nalso adopte this framework by incorporating re-\ntrieval information from two sources (i.e. sparse\nand dense memories) and obtained an improved\nmodel performance.\nData-to-Text Generation\nRecently, retrieval-\naugmented generation has been adapted to the task\nof data-to-text generation. To bridge the gap be-\ntween the structured data and natural language\ntext, Su et al. (2021a) propose a novel retrieval-\naugmented framework.\nSpeciﬁcally, given the\nsource data, a set of candidate texts are ﬁrst re-\ntrieved from a large unlabelled corpus. Then, a\nneural selector is applied to measure the similari-\nties between the source data and candidate texts,\nand extract a set of more ﬁne-grained prototypes\nfrom the candidates. Lastly, a generation model\ntakes the prototypes as input to produce the text\nthat describes the given structured data.\nWhile retrieval-augmented generation has been\nwidely explored in the NLP community, we sug-\ngest that future research could extend this approach\nto tasks that involve data from multiple modali-\nties. For instance, with recent advancements in\nimage-text retrieval (Jia et al., 2021; Radford et al.,\n2021), the structural gap between images and texts\nis largely bridged. Some early studies (Zhang et al.,\n2020) have shown that information retrieved from\nimages could improve the performance of neural\nmachine translation model. Naturally, such meth-\nods could be extended to other multi-modal tasks,\nsuch as image captioning (Karpathy and Li, 2015).\nA similar idea could also be applied to tasks be-\nyond images, such as speech-to-text transcription\n(Gales and Young, 2007).\n6\nFuture Directions\nDespite the current success of retrieval augmented\ntext generation, there is still a long way to go as\ndiscussed in previous sections. We highlight some\ndirections to facilitate the future research as fol-\nlows:\nRetrieval Sensitivity\nThe performance of re-\ntrieval augmented text generation is very sensitive\nto the retrieval quality, i.e., the similarity between\nthe query and the retrieved examples. Currently, re-\ntrieval augmented text generation models perform\nwell when the retrieved examples are very simi-\nlar to the query. However, they are even worse\nthan the generation models without retrieval when\nthe retrieval examples are less similar. Therefore,\nit would be important to exploit new methods to\naddress such an issue on similarity.\nRetrieval Efﬁciency\nGenerally, if one enlarges\nthe retrieval memory to some extent, it would be\npossible to retrieve an example which is very simi-\nlar to the query.Unfortunately, the downside is that\nthe overall inference for the retrieval augmented\ngeneration models is less efﬁcient due the consid-\nerable retrieval overhead. In this sense, it is urgent\nto consider some methods to trade off the retrieval\nmemory size and retrieval efﬁciency, for example,\ndata compression for the retrieval memory.\nLocal vs. Global Optimization\nTheoretically, it\nseems promising to jointly learn retrieval metrics\nand generation models. However, in practice, there\nis an essential gap about the retrieval metric be-\ntween the training and inference phrases. In the\ntraining phase, the loss is locally back-propagated\nto only a few retrieved examples while in the infer-\nence phase the metric is globally conducted among\nall examples in the memory. It would be interesting\nto narrow such a gap when learning a better metric\nfor generation tasks.\nMulti-Modalities\nWith recent advancement in\nimage-text retrieval, directly associating images\nwith relevant text becomes possible. This urges\nresearchers to investigate the possibility of retrieval-\nbased text generation in tasks that involve data from\ndifferent modalities. One typical task is image\ncaptioning. Beyond images, other tasks like speech-\nto-text transcription could potentially beneﬁt from\nretrieval-based generation methods as well.\nDiverse & Controllable Retrieval\nMost of the\nexisting approaches adopt a universal metric for\nretrieval, such as lexical similarities of sentences.\nFuture work should explore how to use customized\nmetrics for retrieval. This can be beneﬁcial for\nmore controlled text generation. For example, in-\nstances with emotions and styles may be more de-\nsirable in the personalized dialogue generation, par-\nallel data that contains speciﬁc terminologies is\nmore helpful']","Retrieval-augmented generation has been adapted to the task of data-to-text generation by proposing a novel retrieval-augmented framework. This framework involves retrieving a set of candidate texts from a large unlabelled corpus based on the source data. A neural selector is then applied to measure the similarities between the source data and candidate texts, and extract more fine-grained prototypes. Finally, a generation model takes these prototypes as input to produce the text that describes the given structured data.",simple,"[{'Published': '2022-02-13', 'Title': 'A Survey on Retrieval-Augmented Text Generation', 'Authors': 'Huayang Li, Yixuan Su, Deng Cai, Yan Wang, Lemao Liu', 'Summary': 'Recently, retrieval-augmented text generation attracted increasing attention\nof the computational linguistics community. Compared with conventional\ngeneration models, retrieval-augmented text generation has remarkable\nadvantages and particularly has achieved state-of-the-art performance in many\nNLP tasks. This paper aims to conduct a survey about retrieval-augmented text\ngeneration. It firstly highlights the generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable approaches according to different tasks\nincluding dialogue response generation, machine translation, and other\ngeneration tasks. Finally, it points out some important directions on top of\nrecent methods to facilitate future research.'}]",True
5,"What are the unresolved questions about constructing and maintaining a high-quality datastore, and how can retrievers and LMs interact better?","['\nExtending\nthe use of retrieval-augmented LMs beyond conventional\nknowledge-centric tasks necessitates the formulation of a\nnew definition for “relevance” of the input query and docu-\nments in the datastore. This is essential for excelling in tasks\nas in those tasks informative text may not exhibit semantic\nor lexical similarity to the input query. Recent works show\nthat few-shot in-context learning demonstrations (Su et al.,\n2023a) or even unlabeled text (Lyu et al., 2023) could boost\nmodel performance on reasoning or language understand-\ning tasks. Yet, what makes certain documents helpful (e.g.,\nunderlying reasoning patterns, or writing style) remains an\nopen question. Acquiring a better understanding of the char-\nacteristics of helpful documents could unlock the potential\nof retrieval-augmented LMs. Furthermore, built upon such\nunderstanding, we should build retrieval systems capable of\ncontextualized retrieval, rather than building task-specific re-\ntrieval pipelines: developing a versatile retriever that adjusts\nits search behavior based on diverse notions of similarity\nwith additional input.\nFor instance, instruction-tuned re-\ntrievers (Asai et al., 2023b; Su et al., 2023b) exemplify this\ndirection.\nReconsidering and improving the datastore.\nWhen it\ncomes to wider, general downstream applications, or con-\nversely more expert-domain tasks, over-reliance on a single,\n7\nReliable, Adaptable, and Attributable Language Models with Retrieval\ngeneral-domain corpus such as Wikipedia may hinder the\ncapability of retrieval-augmented LMs. As discussed in\nSection 4.1.3, the curation and composition of the datas-\ntore significantly impact the final performance. Yet, many\nopen questions exist regarding how to build and ensure high-\nquality and effective datastores. For instance, should we\nintroduce a quality filter to the documents in the datastore,\nas common practice in pre-training data processing (Black\net al., 2022)? How should we balance multiple domains in a\ndatastore (Shao et al., 2023)? Despite the abundance of liter-\nature on what constitutes good LM pre-training data (Long-\npre et al., 2023), there have been limited explorations so far\non what data ought to go into the datastore.\n5.2. Enhancing Retriever-LM Interactions (C2)\nNew architectures beyond input augmentation.\nAs dis-\ncussed, the input augmentation of powerful LMs (e.g., RAG)\ncomes with several limitations that could be addressed by\nmore specialized, integrated architectures, such as output\ninterpolation or intermediate fusion. While recent work\nshows the success of new architectures (Wang et al., 2023b;\nMin et al., 2023b; Lan et al., 2023), compared to massively\npre-trained parametric LMs, their training and model size\nare often smaller, due to high computational costs for pre-\ntraining. Furthermore, approaches that employ a smaller\ngranularity of retrieval (e.g., token level in Section 4.1.1)\npose significant challenges for scaling. We urge collabora-\ntive efforts for scalable, effective architecture designs and\npre-training—While pre-training retrieval-augmented LMs\nis computationally expensive, we hope that we can address\nthat challenge through collaborative multi-institution efforts,\nas in several successful parametric LM pre-training (Work-\nshop et al., 2022; Groeneveld et al., 2024). Recently, Muen-\nnighoff et al. (2024) have introduced generative represen-\ntational instruction tuning to train a single model for both\nretrieval and generative tasks, which allows for significantly\nreducing the latency of RAG by caching representations.\nIncorporating retrieval during LM pre-training.\nOff-\nthe-shelf parametric LMs trained without retrieval compo-\nnents often struggle with leveraging additional context (Shi\net al., 2023a). Pre-training LMs with retrieval has proven to\nbe effective (Guu et al., 2020; Lewis et al., 2020a; Izacard\net al., 2023), but often requires significant additional train-\ning costs, or non-trivial modifications to the standard LM\narchitecture. Recently, Shi et al. (2024) shows that retriev-\ning similar text chunks and reordering pre-training corpora\ncan enhance LMs’ abilities to reason over long sequences or\nperform retrieval augmentation for', 'Asai & Choi, 2021).\nC2: Limited interactions between retrievers and LMs.\nCommon approaches, such as RAG, often straightforwardly\nentail appending retrieved results to the input of pre-trained\n6\nReliable, Adaptable, and Attributable Language Models with Retrieval\nTable 2. Current status of retrieval-augmented LMs and future directions.\nCurrent State of Retrieval-Augmented LMs (§4)\nRoadmap to Advance Retrieval-Augmented LMs (§5)\nC1: Usage of R\n✗Semantic and lexical similarity only\n✓Beyond semantic and lexical similarity\nand D\n✗Single and general-domain corpora\n✓Datastores for wider applications\nC2: Interaction\n✗Limited interactions beyond input augmentations\n✓Architectures with deep LM-retriever interactions\nof R and θ\n✗Lack of joint optimization from the end use\n✓Large-scale joint training techniques\nC3:\nInfrastruc-\ntures for scaling\n✗Lack of standardized libraries beyond RAG\n✓Standardized and open-sourced library for retrieval-\nbased LMs\n& adoptions\n✗Difficulty in large-scale training and inference\n✓Infrastructure for large-scale training and inference\nparametric LMs and adopting input augmentation (Sec-\ntion 4.1.1), due to its simplicity and effectiveness by lever-\naging state-of-the-art parametric LMs.\nHowever, these\nmethods lack close interactions between the retrieval and\nLM components throughout both training and inference.\nThis deficiency amplifies issues such as unsupported gen-\nerations (Gao et al., 2023a) or susceptibility to irrelevant\ncontext, as noted in Yoran et al. (2024); Shi et al. (2023a).\nMoreover, input augmentation increases the context length\nof LMs, leading to an exponential increase in inference\ncosts (Xu et al., 2024). This becomes particularly prob-\nlematic when downstream applications require systems to\nassimilate information from multiple documents (Fan et al.,\n2019). Extended context can also induce LMs to overlook\nsignificant portions of the input (Liu et al., 2023).\nC3: Lack of infrastructure specialized for retrieval-\nbased LMs.\nRelative to parametric LMs, the optimiza-\ntion of retrieval-augmented LM training procedures has\nbeen comparatively under-studied, from both methodolog-\nical and infrastructural standpoints. For instance, open-\nsourced software such as PyTorch FSDP4 or DeepSpeed5\nenable resource-efficient parametric LM pre-training via\ntechniques such as Fully Sharded Data Parallelism (Zhao\net al., 2023b) or Zero Redundancy Optimizers (Rasley et al.,\n2020), respectively. While retrieval-augmented LMs can\ncertainly leverage improvements made to their paramet-\nric components, what remains lacking are focused efforts\nthat address challenges unique to retrieval-augmented LMs.\nSynchronously updating large-scale indexes during training\nintroduces significant computational overhead, and how to\nefficiently update the index under normal computational\nenvironments remains challenging (Section 4.1.2).\nInference in retrieval-augmented LMs can also be sig-\nnificantly more expensive than in standard parametric\nLMs (Mallen et al., 2023), especially if the datastore is\nlarge (e.g., over one trillion tokens). As scaling pre-training\ndata leads to better parametric LMs, some studies empiri-\ncally show that scaling the datastoresis promising (Borgeaud\n4https://pytorch.org/docs/stable/fsdp.\nhtml\n5https://github.com/microsoft/DeepSpeed\net al., 2022). Yet, nearest neighbor searches over billions\nof embeddings without extensive tricks can consume hun-\ndreds of GPUs or prohibitively high RAM usage. Scaling\ncosts thus hinder prior efforts to use larger datastores (Sec-\ntion 4.1.3).\n5. How Can We Further Advance\nRetrieval-Augmented LMs?\nWe believe that the community needs to develop robust intel-\nligent systems based on retrieval-augmented LMs that sur-\npass fully parametric LMs. Here, we present a roadmap to\novercome the technical constraints associated with retrieval-\naugmented LMs discussed in Section 4.2.\n5.1. Rethinking Retrieval and the Datastore (C1)\nBeyond semantic and lexical similarity.']","The unresolved questions about constructing and maintaining a high-quality datastore include whether to introduce a quality filter to the documents, how to balance multiple domains in a datastore, and what data should go into the datastore. Retriever-LM interactions can be improved by developing architectures with deep LM-retriever interactions and implementing large-scale joint training techniques.",multi_context,"[{'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}, {'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}]",True
6,What is the process of extracting training data from large language models and its contribution to their training?,"[' McCandlish, S., Radford, A., Sutskever, I., and\nAmodei, D. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\n2020.\nURL https://proceedings.neurips.\ncc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.\npdf.\nCao, Q., Min, S., Wang, Y., and Hajishirzi, H. BTR: Binary\ntoken representations for efficient retrieval augmented\nlanguage models. In The Twelfth International Confer-\nence on Learning Representations, 2024. URL https:\n//openreview.net/forum?id=3TO3TtnOFl.\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T., Song, D.,\nErlingsson, U., et al.\nExtracting training data from\nlarge language models. In 30th USENIX Security Sym-\nposium, 2021.\nURL https://arxiv.org/abs/\n2012.07805.\nCarlini, N., Ippolito, D., Jagielski, M., Lee, K., Tramer, F.,\nand Zhang, C. Quantifying memorization across neural\nlanguage models. In The Eleventh International Confer-\nence on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=TatRHT_1cK.\nChen, D., Fisch, A., Weston, J., and Bordes, A. Read-\ning Wikipedia to answer open-domain questions.\nIn\nProceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics, 2017. URL https:\n//aclanthology.org/P17-1171.\nChen, T., Wang, H., Chen, S., Yu, W., Ma, K., Zhao,\nX., Yu, D., and Zhang, H. Dense X Retrieval: What\nretrieval granularity should we use?\narXiv preprint\narXiv:2312.06648, 2023a.\nURL https://arxiv.\norg/abs/2312.06648.\nChen, W., Hu, H., Chen, X., Verga, P., and Cohen, W.\nMuRAG: Multimodal retrieval-augmented generator for\nopen question answering over images and text. In Gold-\nberg, Y., Kozareva, Z., and Zhang, Y. (eds.), Proceed-\nings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, 2022. URL https://\naclanthology.org/2022.emnlp-main.375.\nChen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba,\nK., Salvi, F., Pagliardini, M., Fan, S., K¨\nopf, A., Mo-\nhtashami, A., et al. MEDITRON-70B: Scaling medical\npretraining for large language models. arXiv preprint\narXiv:2311.16079, 2023b.\nURL https://arxiv.\norg/abs/2311.16079.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. PaLM: Scaling language modeling\n10\nReliable, Adaptable, and Attributable Language Models with Retrieval\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nURL https://arxiv.org/abs/2204.02311.\nDao, T., Fu, D., Ermon, S., Rudra, A., and R´\ne, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. In Advances in Neural Information Pro-\ncessing Systems, 2022. URL https://openreview.\nnet/forum?id=H4DqfPSibmx.\nDe Cao, N., Aziz, W., and Titov, I.\nEditing fac-\ntual knowledge in language models.\nIn Moens,\nM.-F.,\n']",Extracting training data from large language models is a process that involves generating new training examples by querying the model and using its responses as the ground truth. This data extraction process helps improve the performance and generalization of the language model during training.,multi_context,"[{'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}]",True
7,What is the concept of few-shot learning and its relation to knowledge distillation and retrieval-augmented language models?,"['-main.74.\nIzacard, G. and Grave, E.\nDistilling knowledge from\nreader to retriever for question answering.\nIn In-\nternational Conference on Learning Representations,\n2021b. URL https://openreview.net/forum?\nid=NTEz-6wysdb.\nIzacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski,\nP., Joulin, A., and Grave, E. Unsupervised dense infor-\nmation retrieval with contrastive learning. Transactions\non Machine Learning Research, 2022. URL https:\n//openreview.net/forum?id=jKN1pXi7b0.\nIzacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,\nF., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and\nGrave, E. Atlas: Few-shot learning with retrieval aug-\nmented language models. Journal of Machine Learning\nResearch, 2023. URL http://jmlr.org/papers/\nv24/23-0037.html.\nJang, J., Yoon, D., Yang, S., Cha, S., Lee, M., Logeswaran,\nL., and Seo, M. Knowledge unlearning for mitigating pri-\nvacy risks in language models. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational\nLinguistics, 2023. URL https://aclanthology.\norg/2023.acl-long.805.\nJiang, Z., Xu, F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J.,\nYang, Y., Callan, J., and Neubig, G. Active retrieval aug-\nmented generation. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 2023.\nURL https://aclanthology.org/\n2023.emnlp-main.495.\nJin, X., Zhang, D., Zhu, H., Xiao, W., Li, S.-W., Wei, X.,\nArnold, A., and Ren, X. Lifelong pretraining: Continually\nadapting language models to emerging corpora. In Pro-\nceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2022. URL https://\naclanthology.org/2022.naacl-main.351.\nJohnson, J., Douze, M., and J´\negou, H. Billion-scale similar-\nity search with gpus. arXiv preprint arXiv:1702.08734,\n2017.\nURL https://arxiv.org/abs/1702.\n08734.\nKandpal, N., Deng, H., Roberts, A., Wallace, E.,\nand Raffel, C.\nLarge language models struggle\nto learn long-tail knowledge.\nIn International\nConference on Machine Learning, 2022a.\nURL\nhttps://proceedings.mlr.press/v202/\nkandpal23a/kandpal23a.pdf.\nKandpal, N., Wallace, E., and Raffel, C. Deduplicating train-\ning data mitigates privacy risks in language models. In\nInternational Conference on Machine Learning, 2022b.\nKarpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L.,\nEdunov, S., Chen, D., and Yih, W.-t. Dense passage\nretrieval for open-domain question answering. In Pro-\nceedings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, 2020. URL https://\naclanthology.org/2020.emnlp-main.550.\nKasai, J., Sakaguchi, K., Takahashi, Y., Bras, R. L., Asai,\nA., Yu, X., Radev, D., Smith, N. A., Choi, Y., and Inui,\nK. Realtime qa: What’s the answer right now? In Thirty-\nseventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2023. URL\nhttps://arxiv.org/abs/2207.13332.\nKhandelwal, U., Levy, O., Jurafsky, D., Zettlemoyer,', '.\nRetrieval-\naugmented generation for knowledge-intensive nlp tasks.\nIn Advances in Neural Information Processing Sys-\ntems,\n2020b.\nURL\nhttps://proceedings.\nneurips.cc/paper/2020/file/\n6b493230205f780e1bc26945df7481e5-Paper.\npdf.\nLi, D., Chen, Z., Cho, E., Hao, J., Liu, X., Xing, F., Guo, C.,\nand Liu, Y. Overcoming catastrophic forgetting during do-\nmain adaptation of seq2seq language generation. In Pro-\nceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2022. URL https://\naclanthology.org/2022.naacl-main.398.\nLin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad,\nY., Yih, W.-t., and Chen, X.\nHow to train your\ndragon: Diverse augmentation towards generalizable\ndense retrieval.\nIn Bouamor, H., Pino, J., and Bali,\nK. (eds.), Findings of the Association for Computa-\ntional Linguistics: EMNLP 2023, pp. 6385–6400, Singa-\npore, December 2023. Association for Computational\nLinguistics.\ndoi:\n10.18653/v1/2023.findings-emnlp.\n423. URL https://aclanthology.org/2023.\nfindings-emnlp.423.\nLin, X. V., Chen, X., Chen, M., Shi, W., Lomeli, M.,\nJames, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis,\nM., Zettlemoyer, L., and Yih, S. RA-DIT: Retrieval-\naugmented dual instruction tuning. In The Twelfth In-\nternational Conference on Learning Representations,\n2024. URL https://openreview.net/forum?\nid=22OTbutug9.\nLiu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua,\nM., Petroni, F., and Liang, P. Lost in the middle: How\nlanguage models use long contexts. Transactions of the\nAssociation for Computational Linguistics, 2023. URL\nhttps://arxiv.org/abs/2307.03172.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach, 2020. URL https://openreview.net/\nforum?id=SyxS0T4tvS.\nLongpre, S., Yauney, G., Reif, E., Lee, K., Roberts, A.,\nZoph, B., Zhou, D., Wei, J., Robinson, K., Mimno, D.,\net al. A pretrainer’s guide to training data: Measuring the\neffects of data age, domain coverage, quality, & toxicity.\narXiv preprint arXiv:2305.13169, 2023. URL https:\n//arxiv.org/abs/2305.13169.\nLuo, H., Chuang, Y.-S., Gong, Y., Zhang, T., Kim, Y.,\nWu, X., Fox, D., Meng, H., and Glass, J. Sail: Search-\naugmented instruction learning. In Findings of the As-\n13\nReliable, Adaptable, and Attributable Language Models with Retrieval\nsociation for Computational Linguistics: EMNLP 2023,\n2023. URL https://aclanthology.org/2023.\nfindings-emnlp.242.\nLyu, X., Min, S., Beltagy, I., Zettlemoyer, L., and Hajishirzi,\nH. Z-ICL: Zero-shot in-context learning with pseudo-\ndemonstrations. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics,\n2023. URL https://aclanthology.org/2023.\nacl-long.129.\nMalavi']",The concept of few-shot learning refers to the ability of a model to learn and generalize from a small amount of labeled data. It is related to knowledge distillation and retrieval-augmented language models in the sense that these techniques can be used to improve the performance of few-shot learning models by leveraging external knowledge and information retrieval.,multi_context,"[{'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}, {'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}]",True
8,How can retrieval-augmented LMs help with challenges in identifying and filtering out training instances with potential privacy or copyright-protected data from datasets?,"[' masked tokens. Dur-\ning test time, for an input sequence x, the trained θ predicts\nthe outputs: y = fθ(x), without accessing any external data\nbeyond that of the task at hand.\n2.1. Weaknesses of Parametric LMs\nMounting evidence highlights significant limitations in para-\nmetric LMs. Many such challenges arise from the strategy\nof attempting to store all knowledge within the parameters,\nwhich scaling alone may not adequately address.\nW1: Factual inaccuracies.\nAttempting to memorize all\nthe learned knowledge within the parameters can lead to\nfactual inaccuracies, which are often called hallucinations.\nSeveral recent papers report that even state-of-the-art LMs\nsuch as ChatGPT exhibit hallucinations in the majority of\ntheir outputs (Min et al., 2023a; Mishra et al., 2024). Mallen\net al. (2023); Kandpal et al. (2022a) show that they particu-\nlarly struggle with long-tail knowledge—factual knowledge\nthat is less represented during pre-training—and that scaling\nonly yields minor improvements. Gudibande et al. (2024)\nfind that increasing synthetic labeled data during instruction\ntuning may not improve the factuality of model outputs.\nW2: Difficulty of verifications.\nNot only have LMs\nshown a propensity for hallucinations in their generations,\nbut it is also difficult for practitioners to fact-check their out-\nputs due to a lack of clear attributions or provenance. The\noutputs of powerful LMs are often lengthy, assertive, and\nplausible (Min et al., 2023a), which makes post-hoc attri-\nbutions or factual verification to be challenging and largely\nunsolved tasks (Mishra et al., 2024; Yue et al., 2023).\nW3: Difficulty of opting out certain sequences from the\ndatasets.\nManaging the vast volume of pre-training data\nposes a considerable challenge in identifying and filtering\nout training instances with potential privacy (Brown et al.,\n2022) or copyright-protected data (Lee et al., 2024). Re-\ncent work studies intensive red teaming and safety tuning\nefforts (Touvron et al., 2023b; Perez et al., 2022), unlearn-\ning (Jang et al., 2023) or iterative pre-training of models on\ncorpora after removing certain data (Kandpal et al., 2022b).\nYet, the absence of proper attributions further complicates\nthese endeavors, as tracing back to and eliminating specific\ntraining instances becomes non-trivial (Grosse et al., 2023).\nW4: Computationally expensive costs to adapt.\nAdapt-\ning parametric LMs trained on static unlabeled text (i.e.,\n2\nReliable, Adaptable, and Attributable Language Models with Retrieval\ntext collected at a certain timestamp from the web) re-\nquires continuous training or computationally expensive\npost-adaptation to new data distributions. For instance, their\nparametric knowledge can quickly become obsolete (Long-\npre et al., 2023). While several approaches propose to locate\nand edit certain outdated knowledge (De Cao et al., 2021)\nor conduct efficient continued training (Jin et al., 2022) to\nkeep up with the world, these approaches require additional\ncomputationally expensive learning processes. LMs trained\non widely adopted pre-training corpora often perform well\non general-purpose domains such as news articles (Dodge\net al., 2021), but struggle on expert domains (Taylor et al.,\n2022). Prior work demonstrates the effectiveness of contin-\nued pre-training (Azerbayev et al., 2024; Chen et al., 2023b)\nor instruction tuning (Singhal et al., 2023), albeit at a consid-\nerable computational cost and possibilities of catastrophic\nforgetting (Li et al., 2022).\nW5: Prohibitively large model size.\nNumerous studies\nshowcase the positive impact of model scaling on task per-\nformance (Chowdhery et al., 2022; Wei et al., 2022), and\nthe ability to recall factual knowledge memorized from the\ntraining data (Carlini et al., 2023; Mallen et al., 2023; Kand-\npal et al., 2022a). This trend has prompted the community\nto focus on boosting the model size in pursuit of better\nperformance, at the cost of significant computational chal-\nlenges and environmental concerns (Strubell et al., 2019;\nWeidinger et al., 2022). Despite efforts to enhance effi-\nciency, hosting these massive models, which often exceed a', 'ed LMs\nto supersede parametric LMs as the next generation of LMs\n(Figure 1, bottom), addressing many of the aforementioned\nweaknesses. Unlike parametric LMs—which use large-scale\ntext data only during training—retrieval-augmented LMs\nleverage an external large-scale collection of documents\n(datastore) at inference by selecting relevant documents\nfrom the datastore (Asai et al., 2023a). Retrieval-augmented\nLMs can W1: largely reduce factual errors (Mallen et al.,\n2023), W2: provide better attributions (Gao et al., 2023a),\nW3: enabling flexible opt-in and out of sequences (Min\net al., 2024). By adding or removing data from their datas-\ntores, retrieval-augmented LMs can W4: easily adapt to new\ndistributions (Khandelwal et al., 2020). Lifting the burden\nof memorizing everything in parameters makes them W5:\n1\narXiv:2403.03187v1  [cs.CL]  5 Mar 2024\nReliable, Adaptable, and Attributable Language Models with Retrieval\nmore parameter-efficient (Izacard et al., 2023).\nDespite their considerable potential to significantly improve\nreliability, adaptability, and attributability, their broader\nadoption beyond specific knowledge-intensive tasks (e.g.,\nquestion answering or QA; Chen et al. 2017) is currently\nlimited. We argue that through fundamental advancements\nin architecture, training methodologies, and infrastructure\nfor retrieval-augmented LMs, they can demonstrate sub-\nstantial efficacy across diverse domains. We urge the re-\nsearch community to intensify efforts aimed at overcoming\nthose inherent limitations for their widespread adoption.\nTo facilitate future research, we identify several significant\nchallenges. First, existing approaches primarily leverage\ncontext with high semantic or lexical similarity to the in-\nput (C1), struggling when helpful text is absent in com-\nmon datastores or does not align with conventional rele-\nvance definitions (BehnamGhader et al., 2023; Asai et al.,\n2023b). Second, prepending the retrieved text to the in-\nput of off-the-shelf LMs, which has been widely used re-\ncently, leads to shallow interactions between the retrieval\nand LM components (C2). This often results in unsupported\ngenerations (Gao et al., 2023a), susceptibility to irrelevant\ntext (Yoran et al., 2024), and challenges in handling infor-\nmation from multiple pieces of text (Borgeaud et al., 2022).\nFurthermore, unlike rapid progress for efficient training and\ninference of parametric LMs (Zhao et al., 2023b; Dao et al.,\n2022), there are limited studies and open-sourced efforts to\nenhance the training and inference efficiency of retrieval-\naugmented LMs at scale (C3).\nWe conclude this paper with a roadmap to advance retrieval-\naugmented LMs to foster wider adoption. First, addressing\nthe challenge of finding helpful text for diverse tasks (C1),\nit is important to reconsider the notion of relevance and\nadvance our understanding of what constitutes an effec-\ntive datastore—specifically, exploring the types of infor-\nmation that should be retrieved from various datastores to\nenhance the performance in broader tasks. Then, we sug-\ngest approaches to ensure deeper interactions between the\ntwo components, including architecture, pre-training, and\npost-training adaptations (C2), rather than focusing on sup-\nplementary enhancement of existing parametric LMs. For\nchallenges of scaling (C3), we call for more open-sourced\nand interdisciplinary efforts across hardware, systems, and\nalgorithms to develop infrastructures for training and infer-\nence (e.g., scaling datastore to trillion tokens). By pursuing\nthese avenues, we anticipate unlocking the full capabilities\nof retrieval-augmented LMs and expanding their applica-\ntions across a spectrum of tasks and domains.\n2. How Far Can We Go with Parametric LMs?\nWe first assess the limitations of parametric LMs. Despite\nrapid progress in this area, we argue that parametric LMs\nhave many practical limitations, which in turn pose signifi-\ncant challenges to building reliable intelligent systems.\nDefinition.\nA parametric LM (Figure 1, top) consists of\na set of parameters θ. Given input sequences from a large-\nscale text dataset Dtrain, learnable parameters θ are trained\nto predict the probabilities of future or']",nan,multi_context,"[{'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}, {'Published': '2024-03-05', 'Title': 'Reliable, Adaptable, and Attributable Language Models with Retrieval', 'Authors': 'Akari Asai, Zexuan Zhong, Danqi Chen, Pang Wei Koh, Luke Zettlemoyer, Hannaneh Hajishirzi, Wen-tau Yih', 'Summary': 'Parametric language models (LMs), which are trained on vast amounts of web\ndata, exhibit remarkable flexibility and capability. However, they still face\npractical challenges such as hallucinations, difficulty in adapting to new data\ndistributions, and a lack of verifiability. In this position paper, we advocate\nfor retrieval-augmented LMs to replace parametric LMs as the next generation of\nLMs. By incorporating large-scale datastores during inference,\nretrieval-augmented LMs can be more reliable, adaptable, and attributable.\nDespite their potential, retrieval-augmented LMs have yet to be widely adopted\ndue to several obstacles: specifically, current retrieval-augmented LMs\nstruggle to leverage helpful text beyond knowledge-intensive tasks such as\nquestion answering, have limited interaction between retrieval and LM\ncomponents, and lack the infrastructure for scaling. To address these, we\npropose a roadmap for developing general-purpose retrieval-augmented LMs. This\ninvolves a reconsideration of datastores and retrievers, the exploration of\npipelines with improved retriever-LM interaction, and significant investment in\ninfrastructure for efficient training and inference.'}]",True
9,How does CRAG improve RAG and Self-RAG accuracy with different datasets and LLMs?,"[' on four datasets. The\nmodel coupling the proposed method with standard\nRAG is named CRAG and that coupling with Self-\nRAG is named Self-CRAG. Readers can refer to\nAppendix B.2 for more implementation details of\nour proposed methods. From these results, we can\nconclude the following findings:\nFirst, the proposed method can significantly\nimprove the performance of RAG and Self-RAG.\nSpecifically,\nCRAG\noutperformed RAG by\nmargins of 19.0% accuracy on PopQA, 14.9%\nFactScore on Biography, 36.6% accuracy on\nPubHealth, and 8.1% accuracy on Arc-Challenge\nwhen based on SelfRAG-LLaMA2-7b, as well\nas by margins of 2.1% accuracy on PopQA,\n2.8% FactScore on Biography, and 2.0% on\nArc-Challenge when based on LLaMA2-hf-7b.\nCompared with the current state-of-the-art Self-\nRAG, Self-CRAG outperformed it by margins of\n20.0% accuracy on PopQA, 36.9% FactScore on\nBiography, and 4.0% accuracy on Arc-Challenge\nwhen based on LLaMA2-hf-7b, as well as by\nmargins of 6.9% accuracy on PopQA, 5.0%\nFactScore on Biography, and 2.4% accuracy on\nPubHealth, when based on SelfRAG-LLaMA2-7b.\nThese\nresults\ndemonstrated\nthe\nadaptability\nof CRAG which is plug-and-play and can be\nimplemented into RAG-based approaches.\nSecond, the proposed method demonstrated\ngreat generalizability across a variety of gen-\neration tasks.\nIn particular, these benchmarks\nreported in Table 1 respectively represent different\npractical scenarios including short-form entity\ngeneration (PopQA), long-form generation (Bi-\nography), and closed-set tasks (PubHealth, Arc-\nChallenge). These results verified the consistent\neffectiveness of CRAG. Its versatility across a spec-\ntrum of tasks underscores its robust capabilities and\ngeneralizability across diverse scenarios.\nThird, the proposed method exhibited greater\nflexibility in replacing the underlying LLM gen-\nerator. It can be seen that CRAG still showed\ncompetitive performance when the underlying\nLLMs was changed from SelfRAG-LLaMA2-7b\nto LLaMA2-hf-7b, while the performance of Self-\nLLaMA2-hf-7b SelfRAG-LLaMA2-7b\nCRAG\n47.3\n59.3\nw/o. Correct\n44.5\n58.1\nw/o. Incorrect\n46.8\n58.6\nw/o. Ambiguous\n45.7\n58.5\nSelf-CRAG\n49.0\n61.8\nw/o. Correct\n43.6\n59.6\nw/o. Incorrect\n47.7\n60.8\nw/o. Ambiguous\n48.1\n61.5\nTable 2: Ablation study for removing each single action\non the PopQA dataset in terms of accuracy.\nRAG dropped significantly, even underperforming\nthe standard RAG on several benchmarks. The\nreason for these results is that Self-RAG needs to be\ninstruction-tuned using human or LLM annotated\ndata to learn to output special critic tokens as\nneeded, while this ability is not learned in common\nLLMs. CRAG does not have any requirements\nfor this ability. As you can imagine, when more\nadvanced LLMs are available in the future, they\ncan be coupled with CRAG easily, while additional\ninstruction tuning is still necessary for Self-RAG.\n5.4\nAblation Study\nThe impact of each triggered action.\nTo fur-\nther verify the effectiveness of triggered actions\ndesigned in the retrieval evaluator, ablation tests\nfor removing each single action in the proposed\nmethod were conducted as shown in Table 2.\nEvaluations on the PopQA dataset were conducted\nto demonstrate the performance change in terms of\naccuracy. Specifically, when the action Correct\nor Incorrect was removed, it was merged with\nAmbiguous so that the proportion that originally\ntriggered Correct or Incorrect would trigger\nAmbiguous. On the other hand, when the action\nAmbiguous was removed, there was only one\nthreshold against which all input queries clearly\ntriggered Correct or Incorrect.\nFrom these\nresults, it can be seen that there was a performance']","The proposed method, CRAG, significantly improves the performance of RAG and Self-RAG. CRAG outperformed RAG by margins of 19.0% accuracy on PopQA, 14.9% FactScore on Biography, 36.6% accuracy on PubHealth, and 8.1% accuracy on Arc-Challenge when based on SelfRAG-LLaMA2-7b. It also outperformed RAG by margins of 2.1% accuracy on PopQA, 2.8% FactScore on Biography, and 2.0% accuracy on Arc-Challenge when based on LLaMA2-hf-7b. Compared to Self-RAG, Self-CRAG outperformed it by margins of 20.0% accuracy on PopQA, 36.9% FactScore on Biography, and 4.0% accuracy on Arc-Challenge when based on LLaMA2-hf-7b. It also outperformed Self-RAG by margins of 6.9% accuracy on PopQA, 5.0% FactScore on Biography, and 2.4% accuracy on PubHealth when based on SelfRAG-LLaMA2-7b.",reasoning,"[{'Published': '2024-02-16', 'Title': 'Corrective Retrieval Augmented Generation', 'Authors': 'Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling', 'Summary': 'Large language models (LLMs) inevitably exhibit hallucinations since the\naccuracy of generated texts cannot be secured solely by the parametric\nknowledge they encapsulate. Although retrieval-augmented generation (RAG) is a\npracticable complement to LLMs, it relies heavily on the relevance of retrieved\ndocuments, raising concerns about how the model behaves if retrieval goes\nwrong. To this end, we propose the Corrective Retrieval Augmented Generation\n(CRAG) to improve the robustness of generation. Specifically, a lightweight\nretrieval evaluator is designed to assess the overall quality of retrieved\ndocuments for a query, returning a confidence degree based on which different\nknowledge retrieval actions can be triggered. Since retrieval from static and\nlimited corpora can only return sub-optimal documents, large-scale web searches\nare utilized as an extension for augmenting the retrieval results. Besides, a\ndecompose-then-recompose algorithm is designed for retrieved documents to\nselectively focus on key information and filter out irrelevant information in\nthem. CRAG is plug-and-play and can be seamlessly coupled with various\nRAG-based approaches. Experiments on four datasets covering short- and\nlong-form generation tasks show that CRAG can significantly improve the\nperformance of RAG-based approaches.'}]",True
